#!/usr/bin/env python3
# encoding=UTF-8

# Copyright Â© 2017-2025 Jakub Wilk <jwilk@jwilk.net>
# SPDX-License-Identifier: MIT

import argparse
import fcntl
import getpass
import http.cookiejar
import json
import os
import re
import sqlite3
import sys
import urllib.request

import html2text
import lxml.html

0_0  # Python >= 3.6 is required

prog = argparse.ArgumentParser().prog

def makedirs700(path):
    # TODO: Get rid of this function once
    # https://github.com/python/cpython/issues/86533
    # ("Restore os.makedirs ability to apply mode to all directories created")
    # is fixed.
    if os.path.isdir(path):
        return
    parent = os.path.dirname(path)
    if parent:
        makedirs700(parent)
    try:
        os.mkdir(path, 0o700)
    except OSError:
        if not os.path.isdir(path):
            raise

class Cache:

    @staticmethod
    def _create_cache_dir():
        path = os.getenv('XDG_CACHE_HOME', '')
        if not path.startswith('/'):
            path = os.path.join(os.path.expanduser('~'), '.cache')
        path = os.path.join(path, 'lwlates')
        makedirs700(path)
        return path

    def __init__(self):
        self.dir = self._create_cache_dir()
        self.conn = None

    def __enter__(self):
        db_path = os.path.join(self.dir, 'cache.db')
        conn = self.conn = sqlite3.connect(db_path)
        with conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS data (
                    key TEXT PRIMARY KEY NOT NULL,
                    value TEXT NOT NULL,
                    timestamp TIMESTAMP
                )'''
            )
        # TODO for 2027: remove the JSON -> SQLite migration code
        json_path = os.path.join(self.dir, 'cache.json')
        lock_fd = os.open(self.dir, os.O_RDONLY)
        try:
            try:
                fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
            except BlockingIOError:
                print(f'{prog}: waiting for the lock...', end='', file=sys.stderr)
                sys.stderr.flush()
                fcntl.flock(lock_fd, fcntl.LOCK_EX)
                print('', file=sys.stderr)
            try:
                with open(json_path, 'rt', encoding='UTF-8') as fp:
                    data = json.load(fp)
            except FileNotFoundError:
                data = None
            if data:
                if data.pop('__version__') != 0:
                    data = {}
                with conn:
                    conn.executemany('''
                        INSERT OR IGNORE INTO data(key, value)
                        VALUES(?, ?)''',
                        data.items()
                    )
            if data is not None:
                os.unlink(json_path)
        finally:
            os.close(lock_fd)
        return self

    def __getitem__(self, key):
        with self.conn as conn:
            cur = conn.execute('SELECT value FROM data WHERE key = ?', [key])
            row = cur.fetchone()
            if row is None:
                raise KeyError
            [value] = row
            return value

    def __setitem__(self, key, value):
        with self.conn as conn:
            conn.execute('''
                INSERT OR IGNORE INTO data(key, value, timestamp)
                VALUES (?, ?, DATETIME("now"))''',
                (key, value)
             )

    def __exit__(self, *exc_info):
        self.conn.close()
        self.conn = None

class UserAgent:

    ident = 'lwlates (https://github.com/jwilk/lwlates)'
    base_url = 'https://lwn.net/'
    logging = None

    def __init__(self):
        cookie_jar = http.cookiejar.CookieJar()
        cookie_proc = urllib.request.HTTPCookieProcessor(cookie_jar)
        self.opener = urllib.request.build_opener(cookie_proc)
        self.headers = {
            'User-Agent': self.ident,
        }

    def _request(self, url, *, data):
        url = urllib.parse.urljoin(self.base_url, url)
        if self.logging is not None:
            print('*', url, file=self.logging)
        request = urllib.request.Request(
            url,
            headers=self.headers,
            data=data,
        )
        with self.opener.open(request) as response:
            return response.read()

    def get(self, url):
        return self._request(url, data=None)

    def post(self, url, *, data):
        return self._request(url, data=data)

def html2md(text):
    cvtr = html2text.HTML2Text()
    cvtr.unicode_snob = True
    cvtr.body_width = 999999
    return cvtr.handle(text)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--user', metavar='USER', required=True)
    ap.add_argument('--password-file', metavar='FILE', help='read password from FILE')
    ap.add_argument('--verbose', action='store_true')
    options = ap.parse_args()
    if options.password_file is not None:
        with open(options.password_file, 'rt') as file:
            password = file.readline()
        password = password.rstrip('\n')
    else:
        password = getpass.getpass('LWN.net password: ')
    if not password:
        print(f'{prog}: error: empty password', file=sys.stderr)
        sys.exit(1)
    user = options.user
    ua = UserAgent()
    if options.verbose:
        ua.logging = sys.stderr
    login_data = dict(
        uname=user,
        pword=password,
        target='/',
    )
    login_data = urllib.parse.urlencode(login_data)
    login_data = login_data.encode('ASCII')
    need_logout = True
    data = ua.post('/Login/', data=login_data)
    try:
        doc = lxml.html.document_fromstring(data)
        elt = doc.xpath('//h1')[0]
        if elt.text.startswith('Log in'):
            need_logout = False
            print(f'{prog}: error: could not log in', file=sys.stderr)
            sys.exit(1)
        quser = urllib.parse.quote(user, safe='')
        comments_url = f'/MyAccount/{quser}/comments'
        with Cache() as cache:
            batch_url = f'{comments_url}?n=100'
            while batch_url:
                data = ua.get(batch_url)
                doc = lxml.html.document_fromstring(data)
                batch_url = None
                for elt, _, url, _ in doc.iterlinks():
                    text = elt.text or ''
                    if text.isdecimal():
                        full_url = urllib.parse.urljoin(ua.base_url, url)
                        print(full_url)
                        no = text
                        try:
                            html = cache[no]
                        except LookupError:
                            data = ua.get(url)
                            doc = lxml.html.document_fromstring(data)
                            [elt] = doc.xpath('//div[@class="ArticleText"]')
                            html = lxml.html.tostring(elt, encoding='unicode')
                            cache[no] = html
                        text = html2md(html)
                        text = re.sub(r'\n\s*---\s*\Z', '', text)
                        for line in text.splitlines():
                            print('>', line)
                        print()
                    elif text.startswith('Next ') and url.startswith(f'{comments_url}?'):
                        batch_url = url
    finally:
        if need_logout:
            ua.get('/Login/logout')

if __name__ == '__main__':
    main()

# vim:ts=4 sts=4 sw=4 et
